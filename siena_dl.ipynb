{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2vV9BiuT6zly"
   },
   "source": [
    "### Import all the essential libraries required to build, train, evaluate, and analyze a deep learning model, commonly used in EEG-based seizure detection tasks:\n",
    "\n",
    "\n",
    "-> NumPy is used for numerical computations and handling multi-dimensional arrays,\n",
    "which form the core structure of EEG signal data.\n",
    "\n",
    "->TensorFlow and Keras provide the deep learning framework, where layers and the Model class are used to design neural network architectures such as CNNs and BiLSTMs.\n",
    "\n",
    "->Training optimization is handled using callbacks like EarlyStopping, which stops training when validation performance no longer improves, ModelCheckpoint, which saves the best-performing model during training, and ReduceLROnPlateau, which lowers the learning rate when learning stagnates to improve convergence and prevent overfitting.\n",
    "\n",
    "->Scikit-learn is used for model validation and performance evaluation, with Leave-One-Out cross-validation ensuring robust testing by training on all samples except one iteratively, and evaluation metrics such as accuracy, precision, recall, F1-score, ROC-AUC, and confusion matrix measuring classification effectiveness.\n",
    "\n",
    "->Matplotlib and Seaborn are visualization libraries used to plot EEG signals, training curves, and confusion matrices for interpretability and analysis.\n",
    "\n",
    "->SciPy’s signal module provides signal processing tools such as filtering and spectral analysis, which are crucial for EEG preprocessing.\n",
    "\n",
    "->OS module supports file and directory management for dataset handling, while the warnings module suppresses non-critical runtime warnings to keep outputs clean and focused.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Use GPU\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "# Kill XLA completely\n",
    "os.environ[\"TF_XLA_FLAGS\"] = \"--tf_xla_enable_xla_devices=false\"\n",
    "os.environ[\"XLA_FLAGS\"] = \"\"\n",
    "\n",
    "# Disable MLIR / kernel_gen (CRITICAL)\n",
    "os.environ[\"TF_DISABLE_MLIR_BRIDGE\"] = \"1\"\n",
    "os.environ[\"TF_DISABLE_MLIR_GRAPH_OPTIMIZATIONS\"] = \"1\"\n",
    "\n",
    "# Disable oneDNN (CPU vectorization noise)\n",
    "os.environ[\"TF_ENABLE_ONEDNN_OPTS\"] = \"0\"\n",
    "\n",
    "# Silence logs\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "eNQSEaFJ6f1t"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import callbacks\n",
    "from tensorflow.keras import metrics\n",
    "\n",
    "# IMPORTANT: import layers explicitly\n",
    "from tensorflow.keras.layers import (\n",
    "    Input,\n",
    "    Conv1D,\n",
    "    BatchNormalization,\n",
    "    MaxPooling1D,\n",
    "    Dropout,\n",
    "    Bidirectional,\n",
    "    LSTM,\n",
    "    Dense,\n",
    "    Concatenate\n",
    ")\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    roc_auc_score,\n",
    "    confusion_matrix\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import signal as scipy_signal\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GHGsrbsS7BHV"
   },
   "source": [
    "#GPU configuration:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kGc2kpW0HYmd"
   },
   "source": [
    "###This function configures TensorFlow to efficiently use a GPU for deep learning training and prints detailed diagnostic information in a Jupyter Notebook text (Markdown) cell.\n",
    "\n",
    "\n",
    "->It first checks whether any physical GPUs are available using TensorFlow’s device listing utilities; if GPUs are detected, it enables **memory growth**, which allows TensorFlow to allocate GPU memory dynamically instead of reserving all memory at startup, preventing out-of-memory errors and improving resource sharing.\n",
    "\n",
    "->The function then reports the number and names of detected GPUs. To accelerate training on modern NVIDIA RTX GPUs, it enables **mixed precision training**, which uses both 16-bit (FP16) and 32-bit (FP32) floating-point computations; this leverages **Tensor Cores** to significantly speed up matrix operations while maintaining numerical stability.\n",
    "\n",
    "->**Soft device placement** is enabled so TensorFlow can automatically place operations on the GPU when possible and fall back to the CPU if needed.\n",
    "\n",
    "->The function also retrieves and prints low-level **GPU hardware details** such as compute capability, which helps verify compatibility and performance characteristics.\n",
    "\n",
    "->If GPU configuration is attempted after TensorFlow has already initialized devices, a runtime error is safely caught and reported. When no GPU is available, the function clearly indicates that computation will run on the CPU.\n",
    "\n",
    "->Finally, the function is called immediately so that GPU configuration is applied as soon as the notebook or script is executed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "HZ-BihWn66dB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "GPU CONFIGURATION\n",
      "================================================================================\n",
      "Found 1 GPU(s):\n",
      "  GPU 0: /physical_device:GPU:0\n",
      "Mixed precision enabled: mixed_float16\n",
      " (Utilizes Tensor Cores on RTX GPU for faster training)\n",
      "GPU acceleration enabled\n",
      "GPU Details: {'compute_capability': (8, 6), 'device_name': 'NVIDIA GeForce RTX 3060 Laptop GPU'}\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def configure_gpu():\n",
    "    print(\"=\"*80)\n",
    "    print(\"GPU CONFIGURATION\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    # Check available GPUs\n",
    "    gpus = tf.config.list_physical_devices('GPU')\n",
    "\n",
    "    if gpus:\n",
    "        try:\n",
    "            # Enable memory growth for all GPUs\n",
    "            for gpu in gpus:\n",
    "                tf.config.experimental.set_memory_growth(gpu, True)\n",
    "\n",
    "            print(f\"Found {len(gpus)} GPU(s):\")\n",
    "            for i, gpu in enumerate(gpus):\n",
    "                print(f\"  GPU {i}: {gpu.name}\")\n",
    "\n",
    "            # Enable mixed precision training for RTX GPUs (faster training)\n",
    "            # RTX GPUs have Tensor Cores that accelerate FP16 operations\n",
    "            policy = tf.keras.mixed_precision.Policy('mixed_float16')\n",
    "            tf.keras.mixed_precision.set_global_policy(policy)\n",
    "            print(f\"Mixed precision enabled: {policy.name}\")\n",
    "            print(\" (Utilizes Tensor Cores on RTX GPU for faster training)\")\n",
    "\n",
    "            # Set TensorFlow to use GPU\n",
    "            tf.config.set_soft_device_placement(True)\n",
    "            print(\"GPU acceleration enabled\")\n",
    "\n",
    "            # Display GPU compute capability\n",
    "            gpu_details = tf.config.experimental.get_device_details(gpus[0])\n",
    "            print(f\"GPU Details: {gpu_details}\")\n",
    "\n",
    "        except RuntimeError as e:\n",
    "            print(f\"GPU configuration error: {e}\")\n",
    "    else:\n",
    "        print(\"No GPU found. Running on CPU.\")\n",
    "\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# Call GPU configuration at import time\n",
    "configure_gpu()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ew9iXfeyCBlx"
   },
   "source": [
    "# STEP 1: DATA LOADING AND PREPARATION\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NrJ2yBhpCPIL"
   },
   "source": [
    "###This function loads preprocessed EEG signal data and associated patient information in a format suitable for deep learning models and explains the rationale behind using .npz files.\n",
    "\n",
    " ->It scans a specified directory and loads all .npz files, where each file typically represents one patient and stores multiple related arrays in a single compressed container.\n",
    "\n",
    " ->For each file, the function extracts the patient ID from the filename, then loads the EEG signals—structured as multi-dimensional arrays with dimensions representing samples, channels, and time steps—and the corresponding binary labels indicating seizure or non-seizure events.\n",
    "\n",
    " ->These arrays are then appended to lists so that data from multiple patients can be processed together during training or cross-validation.\n",
    "\n",
    " ->The function also loads demographic embeddings from a separate .npy file, which encode patient-level information (such as age or gender) in a numerical format that can be fused with EEG features in neural networks.\n",
    "\n",
    " ->.npz files are used in deep learning because they efficiently store multiple NumPy arrays in a single compressed file, preserve exact numerical precision without loss, load significantly faster than text-based formats like CSV, and maintain consistent array shapes required by neural networks.\n",
    "\n",
    " ->This makes them ideal for large, high-dimensional data such as EEG signals, where fast I/O, memory efficiency, and structural integrity are critical for stable and reproducible model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "mv8LYR5OCAlr"
   },
   "outputs": [],
   "source": [
    "def load_preprocessed_data(data_dir):\n",
    "    \"\"\"\n",
    "    Loads window-level EEG data.\n",
    "\n",
    "    Returns:\n",
    "        eeg_data    : np.ndarray, shape (N, C, T)\n",
    "        labels      : np.ndarray, shape (N,)\n",
    "        patient_ids : list[str], length N\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Loading preprocessed EEG data...\")\n",
    "\n",
    "    eeg_data = []\n",
    "    labels = []\n",
    "    patient_ids = []\n",
    "\n",
    "    for file in sorted(os.listdir(data_dir)):\n",
    "        if not file.endswith(\".npz\"):\n",
    "            continue\n",
    "\n",
    "        print(f\"Loading {file} ...\")\n",
    "\n",
    "        patient_id = file.split(\".\")[0]\n",
    "        data = np.load(os.path.join(data_dir, file))\n",
    "\n",
    "        X = data[\"X\"]          # (n_windows, C, T)\n",
    "        y = data[\"y\"]          # (n_windows,)\n",
    "\n",
    "        eeg_data.append(X)\n",
    "        labels.append(y)\n",
    "\n",
    "        # ONE patient_id PER WINDOW\n",
    "        patient_ids.extend([patient_id] * len(y))\n",
    "\n",
    "    # FLATTEN TO WINDOW-LEVEL\n",
    "    eeg_data = np.concatenate(eeg_data, axis=0)\n",
    "    labels = np.concatenate(labels, axis=0)\n",
    "\n",
    "    print(f\"Total EEG windows: {len(eeg_data)}\")\n",
    "    print(f\"Total labels:      {len(labels)}\")\n",
    "    print(f\"Total patient IDs: {len(patient_ids)}\")\n",
    "\n",
    "    return eeg_data, labels, patient_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_patient_metadata_map(csv_path):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "        meta_map: dict[str, dict]\n",
    "            patient_id -> raw metadata fields\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    meta_map = {}\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        pid = row[\"patient_id\"]\n",
    "\n",
    "        meta_map[pid] = {\n",
    "            \"age_years\": row[\" age_years\"],\n",
    "            \"gender\": row[\" gender\"],\n",
    "            \"seizure\": row[\" seizure\"],\n",
    "            \"localization\": row[\" localization\"],\n",
    "            \"lateralization\": row[\" lateralization\"],\n",
    "            \"eeg_channel\": row[\" eeg_channel\"],\n",
    "            \"number_seizures\": row[\" number_seizures\"],\n",
    "            \"rec_time_minutes\": row[\" rec_time_minutes\"],\n",
    "        }\n",
    "        \n",
    "    # assert demo_train.ndim == 2, demo_train.shape\n",
    "    # assert demo_train.shape[0] == X_train.shape[0]\n",
    "\n",
    "    return meta_map\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MFf-YIckCvcJ"
   },
   "source": [
    "# STEP 2: TIME-FREQUENCY REPRESENTATION (STFT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "as8ncstGCw6s"
   },
   "source": [
    "###Convert raw EEG time-domain signals into a **time–frequency representation** using the **Short-Time Fourier Transform (STFT)**, which is especially useful for seizure detection because seizures exhibit distinctive frequency patterns that change over time.\n",
    "\n",
    "->The `compute_stft` function takes a single EEG segment (organized by channels) and applies STFT independently to each channel using a specified sampling frequency (`fs`), window length (`nperseg`), and overlap (`noverlap`);\n",
    "\n",
    " ->STFT works by splitting the signal into short, overlapping time windows and applying the Fourier Transform to each window, allowing the model to observe how signal frequencies evolve over time rather than assuming stationarity.\n",
    "\n",
    " ->The complex STFT output is converted to magnitude values using the absolute function, since magnitude spectra capture signal energy distribution and are more meaningful for neural networks.\n",
    "\n",
    " ->The result is a 3D tensor representing channels, frequency bins, and time frames, which closely resembles image-like data suitable for CNNs.\n",
    "\n",
    " ->The `prepare_data_with_stft` function applies this transformation to the entire dataset patient-wise and segment-wise, enabling optional preprocessing control through the `use_stft` flag.\n",
    "\n",
    "  ->Using STFT enhances deep learning performance in EEG analysis because seizures are characterized by transient oscillations and spectral shifts that are difficult to detect in raw time-domain signals but become clearly separable in the time–frequency domain.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "QFpJ9gGHCmgc"
   },
   "outputs": [],
   "source": [
    "def compute_stft(eeg_segment, fs=256, nperseg=128, noverlap=64):\n",
    "    n_channels = eeg_segment.shape[0]\n",
    "    stft_results = []\n",
    "\n",
    "    for ch in range(n_channels):\n",
    "        f, t, Zxx = scipy_signal.stft(eeg_segment[ch], fs=fs,\n",
    "                                       nperseg=nperseg, noverlap=noverlap)\n",
    "        stft_results.append(np.abs(Zxx))\n",
    "\n",
    "    return np.array(stft_results)  # Shape: (n_channels, freq_bins, time_frames)\n",
    "\n",
    "\n",
    "def prepare_data_with_stft(eeg_data, use_stft=True):\n",
    "    if not use_stft:\n",
    "        return eeg_data\n",
    "\n",
    "    print(\"Computing STFT for time-frequency representation...\")\n",
    "    processed_data = []\n",
    "\n",
    "    for patient_data in eeg_data:\n",
    "        patient_stft = []\n",
    "        for segment in patient_data:\n",
    "            stft_segment = compute_stft(segment)\n",
    "            patient_stft.append(stft_segment)\n",
    "        processed_data.append(np.array(patient_stft))\n",
    "\n",
    "    return processed_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l28RUZZ3DDBN"
   },
   "source": [
    "# STEP 3: DATA AUGMENTATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D-NcgnlTDGGJ"
   },
   "source": [
    "###This function performs **data augmentation** on EEG segments to artificially increase dataset diversity and improve the generalization ability of deep learning models.\n",
    "\n",
    "->It first creates a copy of the original EEG segment to preserve the raw data, then applies one of several biologically plausible transformations based on the selected augmentation type.\n",
    "\n",
    "->In the **noise augmentation**, small Gaussian noise is added to simulate real-world recording disturbances such as sensor noise or environmental interference, helping the model become robust to slight signal variations.\n",
    "\n",
    "->The **time-shift augmentation** randomly shifts the EEG signal along the time axis, which teaches the model that seizure patterns are invariant to small temporal misalignments and reduces sensitivity to exact onset positions.\n",
    "\n",
    "->The **channel dropout augmentation** randomly zeros out a subset of EEG channels, mimicking electrode failures or poor contact and encouraging the model to learn spatially distributed patterns rather than relying on a few dominant channels.\n",
    "\n",
    "->Overall, these augmentation strategies help prevent overfitting, improve robustness to noise and missing data, and make the model more reliable when applied to unseen EEG recordings in real clinical settings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "Was_xLcnDQwm"
   },
   "outputs": [],
   "source": [
    "def augment_eeg_data(eeg_segment, augmentation_type='noise'):\n",
    "    augmented = eeg_segment.copy()\n",
    "\n",
    "    if augmentation_type == 'noise':\n",
    "        # Add Gaussian noise (SNR ~20dB)\n",
    "        noise = np.random.normal(0, 0.1, augmented.shape)\n",
    "        augmented = augmented + noise\n",
    "\n",
    "    elif augmentation_type == 'shift':\n",
    "        # Time shifting\n",
    "        shift = np.random.randint(-10, 10)\n",
    "        augmented = np.roll(augmented, shift, axis=-1)\n",
    "\n",
    "    elif augmentation_type == 'dropout':\n",
    "        # Random channel dropout (10% of channels)\n",
    "        n_channels = augmented.shape[0]\n",
    "        dropout_channels = np.random.choice(n_channels,\n",
    "                                           size=int(0.1 * n_channels),\n",
    "                                           replace=False)\n",
    "        augmented[dropout_channels] = 0\n",
    "\n",
    "    return augmented"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G1cJWZQDDgqP"
   },
   "source": [
    "# STEP 4: ATTENTION MECHANISM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KoKwTiFDDdfr"
   },
   "source": [
    "###This custom **AttentionLayer** implements a learnable attention mechanism that allows a neural network to focus on the most informative time steps in sequential EEG data, which is crucial for seizure detection where only certain temporal regions contain discriminative seizure activity.\n",
    "\n",
    "->The layer extends Keras’s base `Layer` class and defines trainable parameters in the `build` method, including a **weight matrix** initialized using Glorot (Xavier) initialization for stable gradient flow and a **bias vector** initialized to zeros.\n",
    "\n",
    "->During the forward pass in the `call` method, the input tensor—structured as batches of time sequences with extracted features—is linearly transformed and passed through a **tanh activation** to produce attention scores that capture the relevance of each time step.\n",
    "\n",
    "->These scores are normalized using a **softmax function** across the temporal dimension, converting them into attention weights that sum to one and represent the relative importance of each time step.\n",
    "\n",
    "->The original inputs are then scaled by these weights through element-wise multiplication, emphasizing seizure-relevant temporal patterns while suppressing less informative background activity.\n",
    "\n",
    "->The `get_config` method ensures that the layer can be properly serialized and reloaded, making it compatible with model saving and deployment workflows in deep learning applications.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "LOupcd3sDX-F"
   },
   "outputs": [],
   "source": [
    "class AttentionLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # input_shape: (batch, time_steps, features)\n",
    "        self.W = self.add_weight(\n",
    "            name=\"attention_weight\",\n",
    "            shape=(input_shape[-1], 1),\n",
    "            initializer=\"glorot_uniform\",\n",
    "            trainable=True\n",
    "        )\n",
    "        self.b = self.add_weight(\n",
    "            name=\"attention_bias\",\n",
    "            shape=(input_shape[1], 1),\n",
    "            initializer=\"zeros\",\n",
    "            trainable=True\n",
    "        )\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # inputs: (batch, time_steps, features)\n",
    "\n",
    "        # Attention scores\n",
    "        e = tf.tanh(tf.matmul(inputs, self.W) + self.b)\n",
    "\n",
    "        # Normalized weights over time\n",
    "        alpha = tf.nn.softmax(e, axis=1)\n",
    "\n",
    "        # Weighted sum over time (THIS is the key fix)\n",
    "        context = tf.reduce_sum(inputs * alpha, axis=1)\n",
    "\n",
    "        return context\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zJvzs8q-EAKz"
   },
   "source": [
    "# STEP 5: CNN-BiLSTM MODEL WITH ATTENTION AND DEMOGRAPHICS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m10HO1_KD8dB"
   },
   "source": [
    "###This function builds a **hybrid CNN–BiLSTM deep learning model with attention and demographic fusion** for EEG-based seizure detection, combining spatial, temporal, and patient-specific information in a single architecture.\n",
    "\n",
    "->The model uses two inputs: one for EEG signals and one for demographic embeddings, allowing it to learn both signal-level and subject-level patterns. If STFT features are used, the EEG input is reshaped to make the time–frequency representation compatible with one-dimensional convolutions.\n",
    "\n",
    "->The **CNN blocks** consist of stacked Conv1D layers with increasing filter sizes that learn local spatial patterns across EEG channels and time, such as rhythmic discharges or spike-like activity, while **batch normalization** stabilizes training, **max pooling** reduces temporal resolution and noise, and **dropout** prevents overfitting.\n",
    "\n",
    "->The extracted features are then passed to **Bidirectional LSTM layers**, which model long-range temporal dependencies in both forward and backward directions, enabling the network to capture seizure onset, evolution, and offset patterns more effectively.\n",
    "\n",
    "->An **attention mechanism** is applied next to dynamically emphasize the most seizure-relevant time segments, improving interpretability and detection accuracy. Global average pooling compresses the attended temporal features into a fixed-length representation.\n",
    "\n",
    "->In parallel, demographic data is processed through a dense layer to learn compact patient embeddings, which are concatenated with EEG-derived features to incorporate patient-specific variability.\n",
    "\n",
    " ->The **classification head** consists of fully connected layers with dropout for robust feature learning, and the final softmax output layer performs binary classification between seizure and non-seizure classes.\n",
    "\n",
    " ->The model is compiled using the **Adam optimizer** for efficient gradient-based learning, **sparse categorical cross-entropy loss** suitable for integer class labels, and evaluation metrics including accuracy, precision, and recall to assess clinical relevance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "d2cAdbLZDolN"
   },
   "outputs": [],
   "source": [
    "def build_cnn_bilstm_model(input_shape, demographic_dim=14, use_stft=False):\n",
    "\n",
    "    from tensorflow.keras import layers, Model\n",
    "    from tensorflow import keras\n",
    "\n",
    "    # --------------------\n",
    "    # Inputs\n",
    "    # --------------------\n",
    "    eeg_input = layers.Input(shape=input_shape, name='eeg_input')\n",
    "    demographic_input = layers.Input(shape=(demographic_dim,), name='demographic_input')\n",
    "\n",
    "    # --------------------\n",
    "    # EEG feature extractor\n",
    "    # --------------------\n",
    "    if use_stft:\n",
    "        x = layers.Reshape((input_shape[0], -1))(eeg_input)\n",
    "    else:\n",
    "        x = eeg_input\n",
    "\n",
    "    x = layers.Conv1D(64, 5, padding='same', activation='relu')(x)\n",
    "    # x = layers.BatchNormalization()(x)\n",
    "    # x = layers.LayerNormalization()(x)\n",
    "#     x = layers.BatchNormalization(\n",
    "#     momentum=0.9,\n",
    "#     epsilon=1e-5,\n",
    "#     fused=False   # ← THIS is the key\n",
    "# )(x)\n",
    "    x = layers.MaxPooling1D(2)(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "\n",
    "    x = layers.Conv1D(128, 5, padding='same', activation='relu')(x)\n",
    "    # x = layers.BatchNormalization()(x)\n",
    "    # x = layers.LayerNormalization()(x)\n",
    "#     x = layers.BatchNormalization(\n",
    "#     momentum=0.9,\n",
    "#     epsilon=1e-5,\n",
    "#     fused=False   # ← THIS is the key\n",
    "# )(x)\n",
    "    x = layers.MaxPooling1D(2)(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "\n",
    "    x = layers.Conv1D(256, 3, padding='same', activation='relu')(x)\n",
    "    # x = layers.BatchNormalization()(x)\n",
    "    # x = layers.LayerNormalization()(x)\n",
    "#     x = layers.BatchNormalization(\n",
    "#     momentum=0.9,\n",
    "#     epsilon=1e-5,\n",
    "#     fused=False   # ← THIS is the key\n",
    "# )(x)\n",
    "    x = layers.MaxPooling1D(2)(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "\n",
    "    # --------------------\n",
    "    # Temporal modeling\n",
    "    # --------------------\n",
    "    x = layers.Bidirectional(layers.LSTM(128, return_sequences=True))(x)\n",
    "    x = layers.Dropout(0.4)(x)\n",
    "\n",
    "    x = layers.Bidirectional(layers.LSTM(64, return_sequences=True))(x)\n",
    "    x = layers.Dropout(0.4)(x)\n",
    "\n",
    "    x = AttentionLayer()(x)\n",
    "    # x = layers.GlobalAveragePooling1D()(x)\n",
    "\n",
    "    # --------------------\n",
    "    # Demographic branch\n",
    "    # --------------------\n",
    "    d = layers.Dense(32, activation='relu')(demographic_input)\n",
    "\n",
    "    # --------------------\n",
    "    # Fusion + classifier\n",
    "    # --------------------\n",
    "    combined = layers.Concatenate()([x, d])\n",
    "\n",
    "    combined = layers.Dense(128, activation='relu')(combined)\n",
    "    combined = layers.Dropout(0.5)(combined)\n",
    "\n",
    "    combined = layers.Dense(64, activation='relu')(combined)\n",
    "    combined = layers.Dropout(0.5)(combined)\n",
    "\n",
    "    output = layers.Dense(2, activation='softmax')(combined)\n",
    "\n",
    "    model = Model(\n",
    "        inputs=[eeg_input, demographic_input],\n",
    "        outputs=output\n",
    "    )\n",
    "\n",
    "    # --------------------\n",
    "    # ✅ CORRECT COMPILE\n",
    "    # --------------------\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(1e-3),\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        metrics=[\"sparse_categorical_accuracy\"],\n",
    "        run_eagerly=True   # REQUIRED on your system\n",
    "    )\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b4wbbznoEWq3"
   },
   "source": [
    "# STEP 6: LEAVE-ONE-PATIENT-OUT (LOPO) CROSS-VALIDATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YeKOYwpMEbAh"
   },
   "source": [
    "###This function implements **Leave-One-Patient-Out (LOPO) cross-validation**, a rigorous evaluation strategy widely used in EEG-based seizure detection to ensure true patient-independent generalization.\n",
    "\n",
    "->In this approach, data from one patient is held out as the test set while the model is trained on data from all remaining patients, and this process is repeated so that each patient serves as the test subject exactly once.\n",
    "\n",
    " ->For every fold, the function concatenates EEG segments and labels from the training patients, aligns demographic embeddings with each EEG segment by repetition, and keeps the held-out patient’s data strictly unseen during training.\n",
    "\n",
    "-> A CNN–BiLSTM–Attention model is then built and trained using GPU acceleration, with multiple **model checkpoints** to save the best-performing models based on validation accuracy and validation loss, as well as periodic snapshots to safeguard against training interruptions.\n",
    "\n",
    " ->**Early stopping** halts training when validation loss stops improving, preventing overfitting, while **learning rate reduction on plateau** improves convergence by lowering the learning rate when optimization stagnates.\n",
    "\n",
    " ->**TensorBoard logging** enables real-time monitoring of GPU usage, losses, and metrics.\n",
    "\n",
    " ->After training, the best model (based on validation accuracy) is reloaded and evaluated on the held-out patient using clinically meaningful metrics such as accuracy, precision, recall, F1-score, AUC–ROC, and the confusion matrix.\n",
    "\n",
    " ->These metrics are stored fold-wise to provide a comprehensive performance summary across all patients.\n",
    "\n",
    " ->Overall, this LOPO framework closely mimics real-world clinical deployment, where models must generalize to entirely unseen patients rather than benefiting from subject-specific data leakage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_metadata(meta):\n",
    "    \"\"\"\n",
    "    Converts raw patient metadata dict into a numeric vector.\n",
    "\n",
    "    Expected keys in meta:\n",
    "        age_years\n",
    "        gender\n",
    "        seizure\n",
    "        localization\n",
    "        lateralization\n",
    "        number_seizures\n",
    "        rec_time_minutes\n",
    "    \"\"\"\n",
    "\n",
    "    # ---------- Numeric ----------\n",
    "    age = meta[\"age_years\"]\n",
    "    num_seizures = meta[\"number_seizures\"]\n",
    "    rec_time = meta[\"rec_time_minutes\"]\n",
    "\n",
    "    # ---------- Binary ----------\n",
    "    gender = 1.0 if meta[\"gender\"] == \"M\" else 0.0\n",
    "\n",
    "    # ---------- Categorical (one-hot) ----------\n",
    "    seizure_types = [\"IAS\", \"GS\", \"FS\", \"AS\"]   # adjust if needed\n",
    "    seizure_oh = [1.0 if meta[\"seizure\"] == s else 0.0 for s in seizure_types]\n",
    "\n",
    "    localizations = [\"T\", \"F\", \"P\", \"O\"]\n",
    "    localization_oh = [1.0 if meta[\"localization\"] == l else 0.0 for l in localizations]\n",
    "\n",
    "    lateralization = [\"L\", \"R\", \"B\"]\n",
    "    lateralization_oh = [1.0 if meta[\"lateralization\"] == lat else 0.0\n",
    "                          for lat in lateralization]\n",
    "\n",
    "    return np.array(\n",
    "        [\n",
    "            age,\n",
    "            gender,\n",
    "            num_seizures,\n",
    "            rec_time,\n",
    "            *seizure_oh,\n",
    "            *localization_oh,\n",
    "            *lateralization_oh\n",
    "        ],\n",
    "        dtype=np.float32\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_patient_id(pid):\n",
    "    \"\"\"\n",
    "    Converts session-level IDs (e.g., PN00-3) to patient-level IDs (PN00)\n",
    "    \"\"\"\n",
    "    return pid.split(\"-\")[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "dXkcQ7z7Ehny"
   },
   "outputs": [],
   "source": [
    "def lopo_cross_validation(\n",
    "    eeg_data,\n",
    "    labels,\n",
    "    patient_ids,\n",
    "    metadata_map,\n",
    "    input_shape,\n",
    "    epochs=50,\n",
    "    batch_size=32\n",
    "):\n",
    "    import numpy as np\n",
    "    import os\n",
    "    import tensorflow as tf\n",
    "    from tensorflow import keras\n",
    "    from sklearn.metrics import (\n",
    "        accuracy_score, precision_score, recall_score,\n",
    "        f1_score, roc_auc_score, confusion_matrix\n",
    "    )\n",
    "\n",
    "    unique_patients = sorted(set(patient_ids))\n",
    "\n",
    "    results = {\n",
    "        \"accuracy\": [],\n",
    "        \"precision\": [],\n",
    "        \"recall\": [],\n",
    "        \"f1_score\": [],\n",
    "        \"auc_roc\": [],\n",
    "        \"confusion_matrices\": []\n",
    "    }\n",
    "\n",
    "    checkpoint_dir = \"./model_checkpoints\"\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Starting LOPO Cross-Validation with {len(unique_patients)} patients\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "\n",
    "    for fold_idx, test_patient in enumerate(unique_patients, start=1):\n",
    "\n",
    "        print(f\"\\n{'─'*80}\")\n",
    "        print(f\"FOLD {fold_idx}/{len(unique_patients)} — TEST PATIENT: {test_patient}\")\n",
    "        print(f\"{'─'*80}\")\n",
    "\n",
    "        # -------------------------------\n",
    "        # Patient-wise split (CORRECT)\n",
    "        # -------------------------------\n",
    "        train_mask = np.array([pid != test_patient for pid in patient_ids])\n",
    "        test_mask  = np.array([pid == test_patient for pid in patient_ids])\n",
    "\n",
    "        X_train = eeg_data[train_mask]\n",
    "        y_train = labels[train_mask]\n",
    "        X_test  = eeg_data[test_mask]\n",
    "        y_test  = labels[test_mask]\n",
    "\n",
    "        # -------------------------------\n",
    "        # Metadata lookup per window\n",
    "        # -------------------------------\n",
    "        demo_train = np.array(\n",
    "            [encode_metadata(metadata_map[normalize_patient_id(pid)])\n",
    "             for pid in np.array(patient_ids)[train_mask]],\n",
    "            dtype=np.float32\n",
    "        )\n",
    "        \n",
    "        demo_test = np.array(\n",
    "            [encode_metadata(metadata_map[normalize_patient_id(pid)])\n",
    "             for pid in np.array(patient_ids)[test_mask]],\n",
    "            dtype=np.float32\n",
    "        )\n",
    "\n",
    "        print(f\"Training samples: {len(X_train)} | Testing samples: {len(X_test)}\")\n",
    "        print(f\"Training seizures: {np.sum(y_train)} | Testing seizures: {np.sum(y_test)}\")\n",
    "\n",
    "        # -------------------------------\n",
    "        # Build model (GPU auto-used)\n",
    "        # -------------------------------\n",
    "        model = build_cnn_bilstm_model(\n",
    "            input_shape=input_shape,\n",
    "            demographic_dim=demo_train.shape[1]\n",
    "        )\n",
    "\n",
    "        # -------------------------------\n",
    "        # Callbacks (research-grade)\n",
    "        # -------------------------------\n",
    "        checkpoint_acc = keras.callbacks.ModelCheckpoint(\n",
    "            filepath=os.path.join(\n",
    "                checkpoint_dir, f\"best_model_fold_{fold_idx}_val_acc.h5\"\n",
    "            ),\n",
    "            monitor=\"val_accuracy\",\n",
    "            mode=\"max\",\n",
    "            save_best_only=True,\n",
    "            verbose=1\n",
    "        )\n",
    "\n",
    "        checkpoint_loss = keras.callbacks.ModelCheckpoint(\n",
    "            filepath=os.path.join(\n",
    "                checkpoint_dir, f\"best_model_fold_{fold_idx}_val_loss.h5\"\n",
    "            ),\n",
    "            monitor=\"val_loss\",\n",
    "            mode=\"min\",\n",
    "            save_best_only=True,\n",
    "            verbose=1\n",
    "        )\n",
    "\n",
    "        checkpoint_epoch = keras.callbacks.ModelCheckpoint(\n",
    "            filepath=os.path.join(\n",
    "                checkpoint_dir,\n",
    "                f\"model_fold_{fold_idx}_epoch_{{epoch:02d}}_val_acc_{{val_accuracy:.4f}}.h5\"\n",
    "            ),\n",
    "            monitor=\"val_accuracy\",\n",
    "            save_best_only=False,\n",
    "            save_weights_only=False,\n",
    "            verbose=0\n",
    "        )\n",
    "\n",
    "        early_stop = keras.callbacks.EarlyStopping(\n",
    "            monitor=\"val_loss\",\n",
    "            patience=10,\n",
    "            restore_best_weights=True,\n",
    "            verbose=1\n",
    "        )\n",
    "\n",
    "        reduce_lr = keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor=\"val_loss\",\n",
    "            factor=0.5,\n",
    "            patience=5,\n",
    "            min_lr=1e-6,\n",
    "            verbose=1\n",
    "        )\n",
    "\n",
    "        tensorboard_cb = keras.callbacks.TensorBoard(\n",
    "            log_dir=os.path.join(checkpoint_dir, f\"logs/fold_{fold_idx}\"),\n",
    "            histogram_freq=1,\n",
    "            update_freq=\"epoch\"\n",
    "        )\n",
    "        print(X_train.shape)\n",
    "        print(y_train.shape)\n",
    "        print(demo_train.shape)\n",
    "\n",
    "\n",
    "        # -------------------------------\n",
    "        # Train\n",
    "        # -------------------------------\n",
    "        X_tr, X_val, d_tr, d_val, y_tr, y_val = train_test_split(\n",
    "            X_train,\n",
    "            demo_train,\n",
    "            y_train,\n",
    "            test_size=0.2,\n",
    "            stratify=y_train,\n",
    "            random_state=42\n",
    "        )\n",
    "\n",
    "        if np.sum(y_train) == 0 or np.sum(y_test) == 0:\n",
    "            print(\"Skipping fold (no seizures in train or test)\")\n",
    "            continue\n",
    "        \n",
    "        model.fit(\n",
    "            [X_tr, d_tr],                 # EEG + demographic inputs\n",
    "            y_tr,                          # sparse labels (0/1)\n",
    "            validation_data=([X_val, d_val], y_val),\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            callbacks=[\n",
    "                checkpoint_acc,\n",
    "                checkpoint_loss,\n",
    "                checkpoint_epoch,\n",
    "                early_stop,\n",
    "                reduce_lr,\n",
    "                tensorboard_cb\n",
    "            ],\n",
    "            verbose=1\n",
    "        )\n",
    "\n",
    "        # -------------------------------\n",
    "        # Evaluate\n",
    "        # -------------------------------\n",
    "        best_model_path = os.path.join(\n",
    "            checkpoint_dir, f\"best_model_fold_{fold_idx}_val_acc.h5\"\n",
    "        )\n",
    "\n",
    "        model = keras.models.load_model(\n",
    "            best_model_path,\n",
    "            custom_objects={\"AttentionLayer\": AttentionLayer}\n",
    "        )\n",
    "\n",
    "        y_probs = model.predict([X_test, demo_test], verbose=0)\n",
    "        y_pred = np.argmax(y_probs, axis=1)\n",
    "\n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "        prec = precision_score(y_test, y_pred, zero_division=0)\n",
    "        rec = recall_score(y_test, y_pred, zero_division=0)\n",
    "        f1 = f1_score(y_test, y_pred, zero_division=0)\n",
    "\n",
    "        try:\n",
    "            auc = roc_auc_score(y_test, y_probs[:, 1])\n",
    "        except ValueError:\n",
    "            auc = 0.0\n",
    "\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "        results[\"accuracy\"].append(acc)\n",
    "        results[\"precision\"].append(prec)\n",
    "        results[\"recall\"].append(rec)\n",
    "        results[\"f1_score\"].append(f1)\n",
    "        results[\"auc_roc\"].append(auc)\n",
    "        results[\"confusion_matrices\"].append(cm)\n",
    "\n",
    "        print(f\"\\nRESULTS — Patient {test_patient}\")\n",
    "        print(f\"Accuracy:  {acc*100:.2f}%\")\n",
    "        print(f\"Precision: {prec*100:.2f}%\")\n",
    "        print(f\"Recall:    {rec*100:.2f}%\")\n",
    "        print(f\"F1-score:  {f1*100:.2f}%\")\n",
    "        print(f\"AUC-ROC:   {auc:.4f}\")\n",
    "        print(\"Confusion Matrix:\")\n",
    "        print(cm)\n",
    "\n",
    "        keras.backend.clear_session()\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y9bA8f4vFAn3"
   },
   "source": [
    "# STEP 7: RESULTS ANALYSIS AND VISUALIZATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3TV7ZBdtFDDv"
   },
   "source": [
    "###This function summarizes and interprets the results obtained from **Leave-One-Patient-Out (LOPO) cross-validation**, providing both quantitative performance statistics and visual insights into model behavior across patients.\n",
    "\n",
    " ->It first computes the **mean and standard deviation** of key evaluation metrics—accuracy, precision, recall, F1-score, and AUC–ROC—across all folds, which reflects how consistently the model performs when tested on unseen patients.\n",
    "\n",
    " ->The function then checks whether a predefined **target accuracy of 95%** has been achieved, offering practical guidance if the target is not met by suggesting strategies such as additional data augmentation or hyperparameter tuning.\n",
    "\n",
    " ->To enhance interpretability, it generates multiple visualizations:\n",
    "  a line plot showing how performance metrics vary across patients,\n",
    "  a box plot illustrating the distribution and variability of metrics,\n",
    "  an **average confusion matrix** that highlights overall classification behavior between seizure and non-seizure classes,\n",
    "  a bar chart displaying **per-patient accuracy** to identify subjects for whom the model performs better or worse.\n",
    "  \n",
    "  ->These plots help detect inter-patient variability, potential bias, and robustness of the model.\n",
    "  \n",
    "  ->Finally, the function saves all visual outputs as a high-resolution image file, making it suitable for reporting, research documentation, and clinical analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "u4Dc4MoUFHUd"
   },
   "outputs": [],
   "source": [
    "def analyze_lopo_results(results, patient_ids):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"OVERALL LOPO CROSS-VALIDATION RESULTS\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "\n",
    "    # Calculate mean and std for each metric\n",
    "    metrics = ['accuracy', 'precision', 'recall', 'f1_score', 'auc_roc']\n",
    "\n",
    "    for metric in metrics:\n",
    "        values = np.array(results[metric])\n",
    "        mean_val = np.mean(values)\n",
    "        std_val = np.std(values)\n",
    "        print(f\"{metric.upper():12s}: {mean_val*100:.2f}% ± {std_val*100:.2f}%\")\n",
    "\n",
    "    print(f\"\\n{'='*80}\")\n",
    "\n",
    "    # Check if target accuracy achieved\n",
    "    mean_accuracy = np.mean(results['accuracy'])\n",
    "    if mean_accuracy >= 0.95:\n",
    "        print(f\"\\n✓ TARGET ACHIEVED: Model accuracy ({mean_accuracy*100:.2f}%) >= 95%\")\n",
    "    else:\n",
    "        print(f\"\\n✗ Target not met: Model accuracy ({mean_accuracy*100:.2f}%) < 95%\")\n",
    "        print(\"   Consider: More data augmentation, hyperparameter tuning, or longer training\")\n",
    "\n",
    "    # Visualization\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "    # Plot 1: Metrics per fold\n",
    "    ax1 = axes[0, 0]\n",
    "    fold_indices = np.arange(1, len(patient_ids) + 1)\n",
    "    ax1.plot(fold_indices, np.array(results['accuracy'])*100, 'o-', label='Accuracy', linewidth=2)\n",
    "    ax1.plot(fold_indices, np.array(results['precision'])*100, 's-', label='Precision', linewidth=2)\n",
    "    ax1.plot(fold_indices, np.array(results['recall'])*100, '^-', label='Recall', linewidth=2)\n",
    "    ax1.plot(fold_indices, np.array(results['f1_score'])*100, 'd-', label='F1-Score', linewidth=2)\n",
    "    ax1.axhline(y=95, color='r', linestyle='--', label='95% Target')\n",
    "    ax1.set_xlabel('Fold (Patient)', fontsize=12)\n",
    "    ax1.set_ylabel('Score (%)', fontsize=12)\n",
    "    ax1.set_title('Performance Metrics per LOPO Fold', fontsize=14, fontweight='bold')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "\n",
    "    # Plot 2: Box plot of metrics\n",
    "    ax2 = axes[0, 1]\n",
    "    box_data = [np.array(results[m])*100 for m in ['accuracy', 'precision', 'recall', 'f1_score']]\n",
    "    bp = ax2.boxplot(box_data, labels=['Accuracy', 'Precision', 'Recall', 'F1-Score'])\n",
    "    ax2.axhline(y=95, color='r', linestyle='--', label='95% Target')\n",
    "    ax2.set_ylabel('Score (%)', fontsize=12)\n",
    "    ax2.set_title('Distribution of Performance Metrics', fontsize=14, fontweight='bold')\n",
    "    ax2.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "    # Plot 3: Average confusion matrix\n",
    "    ax3 = axes[1, 0]\n",
    "    avg_cm = np.mean(results['confusion_matrices'], axis=0)\n",
    "    sns.heatmap(avg_cm, annot=True, fmt='.1f', cmap='Blues', ax=ax3,\n",
    "                xticklabels=['Non-Seizure', 'Seizure'],\n",
    "                yticklabels=['Non-Seizure', 'Seizure'])\n",
    "    ax3.set_title('Average Confusion Matrix', fontsize=14, fontweight='bold')\n",
    "    ax3.set_ylabel('True Label', fontsize=12)\n",
    "    ax3.set_xlabel('Predicted Label', fontsize=12)\n",
    "\n",
    "    # Plot 4: Per-patient performance\n",
    "    ax4 = axes[1, 1]\n",
    "    patient_labels = [f'P{i+1}' for i in range(len(patient_ids))]\n",
    "    x_pos = np.arange(len(patient_labels))\n",
    "    ax4.bar(x_pos, np.array(results['accuracy'])*100, alpha=0.7, color='steelblue')\n",
    "    ax4.axhline(y=95, color='r', linestyle='--', linewidth=2, label='95% Target')\n",
    "    ax4.set_xlabel('Patient', fontsize=12)\n",
    "    ax4.set_ylabel('Accuracy (%)', fontsize=12)\n",
    "    ax4.set_title('Per-Patient Accuracy', fontsize=14, fontweight='bold')\n",
    "    ax4.set_xticks(x_pos)\n",
    "    ax4.set_xticklabels(patient_labels, rotation=45)\n",
    "    ax4.legend()\n",
    "    ax4.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('lopo_results.png', dpi=300, bbox_inches='tight')\n",
    "    print(f\"\\nVisualization saved as 'lopo_results.png'\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UMXMrpdgFR2u"
   },
   "source": [
    "# STEP 8: MAIN EXECUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PZ06U11FFULC"
   },
   "source": [
    "Main execution function for seizure detection with LOPO cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File and parameter configuration\n",
    "METADATA_FILE = \"patient_metadata.csv\"\n",
    "DATA_DIR = \"preprocessed_data_3\"      # Directory with .npz files\n",
    "DEMOGRAPHIC_FILE = \"demographic_embeddings.npy\"\n",
    "\n",
    "USE_STFT = False\n",
    "EPOCHS = 50\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'PN00': {'age_years': 55, 'gender': 'Male', 'seizure': 'IAS', 'localization': 'T', 'lateralization': 'R', 'eeg_channel': 29, 'number_seizures': 5, 'rec_time_minutes': 198}, 'PN01': {'age_years': 46, 'gender': 'Male', 'seizure': 'IAS', 'localization': 'T', 'lateralization': 'L', 'eeg_channel': 29, 'number_seizures': 2, 'rec_time_minutes': 809}, 'PN03': {'age_years': 54, 'gender': 'Male', 'seizure': 'IAS', 'localization': 'T', 'lateralization': 'R', 'eeg_channel': 29, 'number_seizures': 2, 'rec_time_minutes': 752}, 'PN05': {'age_years': 51, 'gender': 'Female', 'seizure': 'IAS', 'localization': 'T', 'lateralization': 'L', 'eeg_channel': 29, 'number_seizures': 3, 'rec_time_minutes': 359}, 'PN06': {'age_years': 36, 'gender': 'Male', 'seizure': 'IAS', 'localization': 'T', 'lateralization': 'L', 'eeg_channel': 29, 'number_seizures': 5, 'rec_time_minutes': 722}, 'PN07': {'age_years': 20, 'gender': 'Female', 'seizure': 'IAS', 'localization': 'T', 'lateralization': 'L', 'eeg_channel': 29, 'number_seizures': 1, 'rec_time_minutes': 523}, 'PN09': {'age_years': 27, 'gender': 'Female', 'seizure': 'IAS', 'localization': 'T', 'lateralization': 'L', 'eeg_channel': 29, 'number_seizures': 3, 'rec_time_minutes': 410}, 'PN10': {'age_years': 25, 'gender': 'Male', 'seizure': 'FBTC', 'localization': 'F', 'lateralization': 'Bilateral', 'eeg_channel': 20, 'number_seizures': 10, 'rec_time_minutes': 1002}, 'PN11': {'age_years': 58, 'gender': 'Female', 'seizure': 'IAS', 'localization': 'T', 'lateralization': 'R', 'eeg_channel': 29, 'number_seizures': 1, 'rec_time_minutes': 145}, 'PN12': {'age_years': 71, 'gender': 'Male', 'seizure': 'IAS', 'localization': 'T', 'lateralization': 'L', 'eeg_channel': 29, 'number_seizures': 4, 'rec_time_minutes': 246}, 'PN13': {'age_years': 34, 'gender': 'Female', 'seizure': 'IAS', 'localization': 'T', 'lateralization': 'L', 'eeg_channel': 29, 'number_seizures': 3, 'rec_time_minutes': 519}, 'PN14': {'age_years': 49, 'gender': 'Male', 'seizure': 'WIAS', 'localization': 'T', 'lateralization': 'L', 'eeg_channel': 29, 'number_seizures': 4, 'rec_time_minutes': 1408}, 'PN16': {'age_years': 41, 'gender': 'Female', 'seizure': 'IAS', 'localization': 'T', 'lateralization': 'L', 'eeg_channel': 29, 'number_seizures': 2, 'rec_time_minutes': 303}, 'PN17': {'age_years': 42, 'gender': 'Male', 'seizure': 'IAS', 'localization': 'T', 'lateralization': 'R', 'eeg_channel': 29, 'number_seizures': 2, 'rec_time_minutes': 308}}\n"
     ]
    }
   ],
   "source": [
    "metadata_map = build_patient_metadata_map(METADATA_FILE)\n",
    "print(metadata_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading preprocessed EEG data...\n",
      "Loading PN00-1.npz ...\n",
      "Loading PN00-2.npz ...\n",
      "Loading PN00-3.npz ...\n",
      "Loading PN00-4.npz ...\n",
      "Loading PN00-5.npz ...\n",
      "Loading PN01-1.npz ...\n",
      "Loading PN03-1.npz ...\n",
      "Loading PN03-2.npz ...\n",
      "Loading PN05-2.npz ...\n",
      "Loading PN05-3.npz ...\n",
      "Loading PN05-4.npz ...\n",
      "Loading PN06-1.npz ...\n",
      "Loading PN06-2.npz ...\n",
      "Loading PN06-3.npz ...\n",
      "Loading PN06-4.npz ...\n",
      "Loading PN06-5.npz ...\n",
      "Loading PN07-1.npz ...\n",
      "Loading PN09-1.npz ...\n",
      "Loading PN09-2.npz ...\n",
      "Loading PN09-3.npz ...\n",
      "Loading PN10-1.npz ...\n",
      "Loading PN10-10.npz ...\n",
      "Loading PN10-2.npz ...\n",
      "Loading PN10-3.npz ...\n",
      "Loading PN10-4.5.6.npz ...\n",
      "Loading PN10-7.8.9.npz ...\n",
      "Loading PN11-1.npz ...\n",
      "Loading PN12-1.2.npz ...\n",
      "Loading PN12-3.npz ...\n",
      "Loading PN12-4.npz ...\n",
      "Loading PN13-1.npz ...\n",
      "Loading PN13-2.npz ...\n",
      "Loading PN13-3.npz ...\n",
      "Loading PN14-1.npz ...\n",
      "Loading PN14-2.npz ...\n",
      "Loading PN14-3.npz ...\n",
      "Loading PN14-4.npz ...\n",
      "Loading PN16-1.npz ...\n",
      "Loading PN16-2.npz ...\n",
      "Loading PN17-1.npz ...\n",
      "Loading PN17-2.npz ...\n",
      "Total EEG windows: 4920\n",
      "Total labels:      4920\n",
      "Total patient IDs: 4920\n"
     ]
    }
   ],
   "source": [
    "# eeg_data, labels, demographics, patient_ids = load_preprocessed_data(\n",
    "#     DATA_DIR,\n",
    "#     DEMOGRAPHIC_FILE\n",
    "# )\n",
    "eeg_data, labels, patient_ids = load_preprocessed_data(DATA_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4920, 27, 1280)\n",
      "(4920,)\n",
      "4920\n"
     ]
    }
   ],
   "source": [
    "print(eeg_data.shape)\n",
    "print(labels.shape)\n",
    "print(len(patient_ids))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 4920\n",
      "Total seizure samples: 0\n",
      "Unique labels: (array([0], dtype=int8), array([4920]))\n"
     ]
    }
   ],
   "source": [
    "print(\"Total samples:\", len(labels))\n",
    "print(\"Total seizure samples:\", labels.sum())\n",
    "print(\"Unique labels:\", np.unique(labels, return_counts=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'PN00-1': 120, 'PN00-2': 120, 'PN00-3': 120, 'PN00-4': 120, 'PN00-5': 120, 'PN01-1': 120, 'PN03-1': 120, 'PN03-2': 120, 'PN05-2': 120, 'PN05-3': 120, 'PN05-4': 120, 'PN06-1': 120, 'PN06-2': 120, 'PN06-3': 120, 'PN06-4': 120, 'PN06-5': 120, 'PN07-1': 120, 'PN09-1': 120, 'PN09-2': 120, 'PN09-3': 120, 'PN10-1': 120, 'PN10-10': 120, 'PN10-2': 120, 'PN10-3': 120, 'PN10-4': 120, 'PN10-7': 120, 'PN11-1': 120, 'PN12-1': 120, 'PN12-3': 120, 'PN12-4': 120, 'PN13-1': 120, 'PN13-2': 120, 'PN13-3': 120, 'PN14-1': 120, 'PN14-2': 120, 'PN14-3': 120, 'PN14-4': 120, 'PN16-1': 120, 'PN16-2': 120, 'PN17-1': 120, 'PN17-2': 120})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "print(Counter(patient_ids))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing metadata for: {'PN13-1', 'PN10-10', 'PN07-1', 'PN09-2', 'PN14-2', 'PN14-4', 'PN12-3', 'PN06-2', 'PN10-7', 'PN06-3', 'PN03-1', 'PN10-4', 'PN14-3', 'PN11-1', 'PN06-5', 'PN16-1', 'PN17-2', 'PN13-3', 'PN00-1', 'PN01-1', 'PN03-2', 'PN05-2', 'PN10-2', 'PN10-3', 'PN12-1', 'PN00-3', 'PN05-3', 'PN05-4', 'PN06-4', 'PN13-2', 'PN00-5', 'PN10-1', 'PN09-1', 'PN06-1', 'PN00-4', 'PN17-1', 'PN16-2', 'PN09-3', 'PN14-1', 'PN12-4', 'PN00-2'}\n"
     ]
    }
   ],
   "source": [
    "missing = set(patient_ids) - set(metadata_map.keys())\n",
    "print(\"Missing metadata for:\", missing)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_STFT:\n",
    "    eeg_data = prepare_data_with_stft(eeg_data, use_stft=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape per sample: (1280,)\n"
     ]
    }
   ],
   "source": [
    "# Assumes all patients have same shape: (n_samples, n_channels, time_steps)\n",
    "input_shape = eeg_data[0].shape[1:]\n",
    "print(\"Input shape per sample:\", input_shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Starting LOPO Cross-Validation with 41 patients\n",
      "================================================================================\n",
      "\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "FOLD 1/41 — TEST PATIENT: PN00-1\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "Training samples: 4800 | Testing samples: 120\n",
      "Training seizures: 0 | Testing seizures: 0\n",
      "(4800, 27, 1280)\n",
      "(4800,)\n",
      "(4800, 15)\n",
      "Skipping fold (no seizures in train or test)\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "FOLD 2/41 — TEST PATIENT: PN00-2\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "Training samples: 4800 | Testing samples: 120\n",
      "Training seizures: 0 | Testing seizures: 0\n",
      "(4800, 27, 1280)\n",
      "(4800,)\n",
      "(4800, 15)\n",
      "Skipping fold (no seizures in train or test)\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "FOLD 3/41 — TEST PATIENT: PN00-3\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "Training samples: 4800 | Testing samples: 120\n",
      "Training seizures: 0 | Testing seizures: 0\n",
      "(4800, 27, 1280)\n",
      "(4800,)\n",
      "(4800, 15)\n",
      "Skipping fold (no seizures in train or test)\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "FOLD 4/41 — TEST PATIENT: PN00-4\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "Training samples: 4800 | Testing samples: 120\n",
      "Training seizures: 0 | Testing seizures: 0\n",
      "(4800, 27, 1280)\n",
      "(4800,)\n",
      "(4800, 15)\n",
      "Skipping fold (no seizures in train or test)\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "FOLD 5/41 — TEST PATIENT: PN00-5\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "Training samples: 4800 | Testing samples: 120\n",
      "Training seizures: 0 | Testing seizures: 0\n",
      "(4800, 27, 1280)\n",
      "(4800,)\n",
      "(4800, 15)\n",
      "Skipping fold (no seizures in train or test)\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "FOLD 6/41 — TEST PATIENT: PN01-1\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "Training samples: 4800 | Testing samples: 120\n",
      "Training seizures: 0 | Testing seizures: 0\n",
      "(4800, 27, 1280)\n",
      "(4800,)\n",
      "(4800, 15)\n",
      "Skipping fold (no seizures in train or test)\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "FOLD 7/41 — TEST PATIENT: PN03-1\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "Training samples: 4800 | Testing samples: 120\n",
      "Training seizures: 0 | Testing seizures: 0\n",
      "(4800, 27, 1280)\n",
      "(4800,)\n",
      "(4800, 15)\n",
      "Skipping fold (no seizures in train or test)\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "FOLD 8/41 — TEST PATIENT: PN03-2\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "Training samples: 4800 | Testing samples: 120\n",
      "Training seizures: 0 | Testing seizures: 0\n",
      "(4800, 27, 1280)\n",
      "(4800,)\n",
      "(4800, 15)\n",
      "Skipping fold (no seizures in train or test)\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "FOLD 9/41 — TEST PATIENT: PN05-2\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "Training samples: 4800 | Testing samples: 120\n",
      "Training seizures: 0 | Testing seizures: 0\n",
      "(4800, 27, 1280)\n",
      "(4800,)\n",
      "(4800, 15)\n",
      "Skipping fold (no seizures in train or test)\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "FOLD 10/41 — TEST PATIENT: PN05-3\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "Training samples: 4800 | Testing samples: 120\n",
      "Training seizures: 0 | Testing seizures: 0\n",
      "(4800, 27, 1280)\n",
      "(4800,)\n",
      "(4800, 15)\n",
      "Skipping fold (no seizures in train or test)\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "FOLD 11/41 — TEST PATIENT: PN05-4\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "Training samples: 4800 | Testing samples: 120\n",
      "Training seizures: 0 | Testing seizures: 0\n",
      "(4800, 27, 1280)\n",
      "(4800,)\n",
      "(4800, 15)\n",
      "Skipping fold (no seizures in train or test)\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "FOLD 12/41 — TEST PATIENT: PN06-1\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "Training samples: 4800 | Testing samples: 120\n",
      "Training seizures: 0 | Testing seizures: 0\n",
      "(4800, 27, 1280)\n",
      "(4800,)\n",
      "(4800, 15)\n",
      "Skipping fold (no seizures in train or test)\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "FOLD 13/41 — TEST PATIENT: PN06-2\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "Training samples: 4800 | Testing samples: 120\n",
      "Training seizures: 0 | Testing seizures: 0\n",
      "(4800, 27, 1280)\n",
      "(4800,)\n",
      "(4800, 15)\n",
      "Skipping fold (no seizures in train or test)\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "FOLD 14/41 — TEST PATIENT: PN06-3\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "Training samples: 4800 | Testing samples: 120\n",
      "Training seizures: 0 | Testing seizures: 0\n",
      "(4800, 27, 1280)\n",
      "(4800,)\n",
      "(4800, 15)\n",
      "Skipping fold (no seizures in train or test)\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "FOLD 15/41 — TEST PATIENT: PN06-4\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "Training samples: 4800 | Testing samples: 120\n",
      "Training seizures: 0 | Testing seizures: 0\n",
      "(4800, 27, 1280)\n",
      "(4800,)\n",
      "(4800, 15)\n",
      "Skipping fold (no seizures in train or test)\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "FOLD 16/41 — TEST PATIENT: PN06-5\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "Training samples: 4800 | Testing samples: 120\n",
      "Training seizures: 0 | Testing seizures: 0\n",
      "(4800, 27, 1280)\n",
      "(4800,)\n",
      "(4800, 15)\n",
      "Skipping fold (no seizures in train or test)\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "FOLD 17/41 — TEST PATIENT: PN07-1\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "Training samples: 4800 | Testing samples: 120\n",
      "Training seizures: 0 | Testing seizures: 0\n",
      "(4800, 27, 1280)\n",
      "(4800,)\n",
      "(4800, 15)\n",
      "Skipping fold (no seizures in train or test)\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "FOLD 18/41 — TEST PATIENT: PN09-1\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "Training samples: 4800 | Testing samples: 120\n",
      "Training seizures: 0 | Testing seizures: 0\n",
      "(4800, 27, 1280)\n",
      "(4800,)\n",
      "(4800, 15)\n",
      "Skipping fold (no seizures in train or test)\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "FOLD 19/41 — TEST PATIENT: PN09-2\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "Training samples: 4800 | Testing samples: 120\n",
      "Training seizures: 0 | Testing seizures: 0\n",
      "(4800, 27, 1280)\n",
      "(4800,)\n",
      "(4800, 15)\n",
      "Skipping fold (no seizures in train or test)\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "FOLD 20/41 — TEST PATIENT: PN09-3\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "Training samples: 4800 | Testing samples: 120\n",
      "Training seizures: 0 | Testing seizures: 0\n",
      "(4800, 27, 1280)\n",
      "(4800,)\n",
      "(4800, 15)\n",
      "Skipping fold (no seizures in train or test)\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "FOLD 21/41 — TEST PATIENT: PN10-1\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "Training samples: 4800 | Testing samples: 120\n",
      "Training seizures: 0 | Testing seizures: 0\n",
      "(4800, 27, 1280)\n",
      "(4800,)\n",
      "(4800, 15)\n",
      "Skipping fold (no seizures in train or test)\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "FOLD 22/41 — TEST PATIENT: PN10-10\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "Training samples: 4800 | Testing samples: 120\n",
      "Training seizures: 0 | Testing seizures: 0\n",
      "(4800, 27, 1280)\n",
      "(4800,)\n",
      "(4800, 15)\n",
      "Skipping fold (no seizures in train or test)\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "FOLD 23/41 — TEST PATIENT: PN10-2\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "Training samples: 4800 | Testing samples: 120\n",
      "Training seizures: 0 | Testing seizures: 0\n",
      "(4800, 27, 1280)\n",
      "(4800,)\n",
      "(4800, 15)\n",
      "Skipping fold (no seizures in train or test)\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "FOLD 24/41 — TEST PATIENT: PN10-3\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "Training samples: 4800 | Testing samples: 120\n",
      "Training seizures: 0 | Testing seizures: 0\n",
      "(4800, 27, 1280)\n",
      "(4800,)\n",
      "(4800, 15)\n",
      "Skipping fold (no seizures in train or test)\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "FOLD 25/41 — TEST PATIENT: PN10-4\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "Training samples: 4800 | Testing samples: 120\n",
      "Training seizures: 0 | Testing seizures: 0\n",
      "(4800, 27, 1280)\n",
      "(4800,)\n",
      "(4800, 15)\n",
      "Skipping fold (no seizures in train or test)\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "FOLD 26/41 — TEST PATIENT: PN10-7\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "Training samples: 4800 | Testing samples: 120\n",
      "Training seizures: 0 | Testing seizures: 0\n",
      "(4800, 27, 1280)\n",
      "(4800,)\n",
      "(4800, 15)\n",
      "Skipping fold (no seizures in train or test)\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "FOLD 27/41 — TEST PATIENT: PN11-1\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "Training samples: 4800 | Testing samples: 120\n",
      "Training seizures: 0 | Testing seizures: 0\n",
      "(4800, 27, 1280)\n",
      "(4800,)\n",
      "(4800, 15)\n",
      "Skipping fold (no seizures in train or test)\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "FOLD 28/41 — TEST PATIENT: PN12-1\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "Training samples: 4800 | Testing samples: 120\n",
      "Training seizures: 0 | Testing seizures: 0\n",
      "(4800, 27, 1280)\n",
      "(4800,)\n",
      "(4800, 15)\n",
      "Skipping fold (no seizures in train or test)\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "FOLD 29/41 — TEST PATIENT: PN12-3\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "Training samples: 4800 | Testing samples: 120\n",
      "Training seizures: 0 | Testing seizures: 0\n",
      "(4800, 27, 1280)\n",
      "(4800,)\n",
      "(4800, 15)\n",
      "Skipping fold (no seizures in train or test)\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "FOLD 30/41 — TEST PATIENT: PN12-4\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "Training samples: 4800 | Testing samples: 120\n",
      "Training seizures: 0 | Testing seizures: 0\n",
      "(4800, 27, 1280)\n",
      "(4800,)\n",
      "(4800, 15)\n",
      "Skipping fold (no seizures in train or test)\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "FOLD 31/41 — TEST PATIENT: PN13-1\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "Training samples: 4800 | Testing samples: 120\n",
      "Training seizures: 0 | Testing seizures: 0\n",
      "(4800, 27, 1280)\n",
      "(4800,)\n",
      "(4800, 15)\n",
      "Skipping fold (no seizures in train or test)\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "FOLD 32/41 — TEST PATIENT: PN13-2\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "Training samples: 4800 | Testing samples: 120\n",
      "Training seizures: 0 | Testing seizures: 0\n",
      "(4800, 27, 1280)\n",
      "(4800,)\n",
      "(4800, 15)\n",
      "Skipping fold (no seizures in train or test)\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "FOLD 33/41 — TEST PATIENT: PN13-3\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "Training samples: 4800 | Testing samples: 120\n",
      "Training seizures: 0 | Testing seizures: 0\n",
      "(4800, 27, 1280)\n",
      "(4800,)\n",
      "(4800, 15)\n",
      "Skipping fold (no seizures in train or test)\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "FOLD 34/41 — TEST PATIENT: PN14-1\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "Training samples: 4800 | Testing samples: 120\n",
      "Training seizures: 0 | Testing seizures: 0\n",
      "(4800, 27, 1280)\n",
      "(4800,)\n",
      "(4800, 15)\n",
      "Skipping fold (no seizures in train or test)\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "FOLD 35/41 — TEST PATIENT: PN14-2\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "Training samples: 4800 | Testing samples: 120\n",
      "Training seizures: 0 | Testing seizures: 0\n",
      "(4800, 27, 1280)\n",
      "(4800,)\n",
      "(4800, 15)\n",
      "Skipping fold (no seizures in train or test)\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "FOLD 36/41 — TEST PATIENT: PN14-3\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "Training samples: 4800 | Testing samples: 120\n",
      "Training seizures: 0 | Testing seizures: 0\n",
      "(4800, 27, 1280)\n",
      "(4800,)\n",
      "(4800, 15)\n",
      "Skipping fold (no seizures in train or test)\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "FOLD 37/41 — TEST PATIENT: PN14-4\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "Training samples: 4800 | Testing samples: 120\n",
      "Training seizures: 0 | Testing seizures: 0\n",
      "(4800, 27, 1280)\n",
      "(4800,)\n",
      "(4800, 15)\n",
      "Skipping fold (no seizures in train or test)\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "FOLD 38/41 — TEST PATIENT: PN16-1\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "Training samples: 4800 | Testing samples: 120\n",
      "Training seizures: 0 | Testing seizures: 0\n",
      "(4800, 27, 1280)\n",
      "(4800,)\n",
      "(4800, 15)\n",
      "Skipping fold (no seizures in train or test)\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "FOLD 39/41 — TEST PATIENT: PN16-2\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "Training samples: 4800 | Testing samples: 120\n",
      "Training seizures: 0 | Testing seizures: 0\n",
      "(4800, 27, 1280)\n",
      "(4800,)\n",
      "(4800, 15)\n",
      "Skipping fold (no seizures in train or test)\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "FOLD 40/41 — TEST PATIENT: PN17-1\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "Training samples: 4800 | Testing samples: 120\n",
      "Training seizures: 0 | Testing seizures: 0\n",
      "(4800, 27, 1280)\n",
      "(4800,)\n",
      "(4800, 15)\n",
      "Skipping fold (no seizures in train or test)\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "FOLD 41/41 — TEST PATIENT: PN17-2\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "Training samples: 4800 | Testing samples: 120\n",
      "Training seizures: 0 | Testing seizures: 0\n",
      "(4800, 27, 1280)\n",
      "(4800,)\n",
      "(4800, 15)\n",
      "Skipping fold (no seizures in train or test)\n"
     ]
    }
   ],
   "source": [
    "results = lopo_cross_validation(\n",
    "    eeg_data=eeg_data,\n",
    "    labels=labels,\n",
    "    patient_ids=patient_ids,\n",
    "    metadata_map=metadata_map,\n",
    "    input_shape=eeg_data.shape[1:],\n",
    "    epochs=50,\n",
    "    batch_size=32\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "OVERALL LOPO CROSS-VALIDATION RESULTS\n",
      "================================================================================\n",
      "\n",
      "ACCURACY    : nan% ± nan%\n",
      "PRECISION   : nan% ± nan%\n",
      "RECALL      : nan% ± nan%\n",
      "F1_SCORE    : nan% ± nan%\n",
      "AUC_ROC     : nan% ± nan%\n",
      "\n",
      "================================================================================\n",
      "\n",
      "✗ Target not met: Model accuracy (nan%) < 95%\n",
      "   Consider: More data augmentation, hyperparameter tuning, or longer training\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "x and y must have same first dimension, but have shapes (4920,) and (0,)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43manalyze_lopo_results\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresults\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatient_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m + \u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m80\u001b[39m)\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mLOPO Cross-Validation Complete!\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 31\u001b[39m, in \u001b[36manalyze_lopo_results\u001b[39m\u001b[34m(results, patient_ids)\u001b[39m\n\u001b[32m     29\u001b[39m ax1 = axes[\u001b[32m0\u001b[39m, \u001b[32m0\u001b[39m]\n\u001b[32m     30\u001b[39m fold_indices = np.arange(\u001b[32m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(patient_ids) + \u001b[32m1\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m \u001b[43max1\u001b[49m\u001b[43m.\u001b[49m\u001b[43mplot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfold_indices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresults\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43maccuracy\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m*\u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mo-\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mAccuracy\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlinewidth\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     32\u001b[39m ax1.plot(fold_indices, np.array(results[\u001b[33m'\u001b[39m\u001b[33mprecision\u001b[39m\u001b[33m'\u001b[39m])*\u001b[32m100\u001b[39m, \u001b[33m'\u001b[39m\u001b[33ms-\u001b[39m\u001b[33m'\u001b[39m, label=\u001b[33m'\u001b[39m\u001b[33mPrecision\u001b[39m\u001b[33m'\u001b[39m, linewidth=\u001b[32m2\u001b[39m)\n\u001b[32m     33\u001b[39m ax1.plot(fold_indices, np.array(results[\u001b[33m'\u001b[39m\u001b[33mrecall\u001b[39m\u001b[33m'\u001b[39m])*\u001b[32m100\u001b[39m, \u001b[33m'\u001b[39m\u001b[33m^-\u001b[39m\u001b[33m'\u001b[39m, label=\u001b[33m'\u001b[39m\u001b[33mRecall\u001b[39m\u001b[33m'\u001b[39m, linewidth=\u001b[32m2\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/matplotlib/axes/_axes.py:1777\u001b[39m, in \u001b[36mAxes.plot\u001b[39m\u001b[34m(self, scalex, scaley, data, *args, **kwargs)\u001b[39m\n\u001b[32m   1534\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1535\u001b[39m \u001b[33;03mPlot y versus x as lines and/or markers.\u001b[39;00m\n\u001b[32m   1536\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   1774\u001b[39m \u001b[33;03m(``'green'``) or hex strings (``'#008000'``).\u001b[39;00m\n\u001b[32m   1775\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1776\u001b[39m kwargs = cbook.normalize_kwargs(kwargs, mlines.Line2D)\n\u001b[32m-> \u001b[39m\u001b[32m1777\u001b[39m lines = [*\u001b[38;5;28mself\u001b[39m._get_lines(\u001b[38;5;28mself\u001b[39m, *args, data=data, **kwargs)]\n\u001b[32m   1778\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m lines:\n\u001b[32m   1779\u001b[39m     \u001b[38;5;28mself\u001b[39m.add_line(line)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/matplotlib/axes/_base.py:297\u001b[39m, in \u001b[36m_process_plot_var_args.__call__\u001b[39m\u001b[34m(self, axes, data, return_kwargs, *args, **kwargs)\u001b[39m\n\u001b[32m    295\u001b[39m     this += args[\u001b[32m0\u001b[39m],\n\u001b[32m    296\u001b[39m     args = args[\u001b[32m1\u001b[39m:]\n\u001b[32m--> \u001b[39m\u001b[32m297\u001b[39m \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_plot_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    298\u001b[39m \u001b[43m    \u001b[49m\u001b[43maxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mambiguous_fmt_datakey\u001b[49m\u001b[43m=\u001b[49m\u001b[43mambiguous_fmt_datakey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    299\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_kwargs\u001b[49m\n\u001b[32m    300\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/matplotlib/axes/_base.py:494\u001b[39m, in \u001b[36m_process_plot_var_args._plot_args\u001b[39m\u001b[34m(self, axes, tup, kwargs, return_kwargs, ambiguous_fmt_datakey)\u001b[39m\n\u001b[32m    491\u001b[39m     axes.yaxis.update_units(y)\n\u001b[32m    493\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m x.shape[\u001b[32m0\u001b[39m] != y.shape[\u001b[32m0\u001b[39m]:\n\u001b[32m--> \u001b[39m\u001b[32m494\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mx and y must have same first dimension, but \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    495\u001b[39m                      \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mhave shapes \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    496\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m x.ndim > \u001b[32m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m y.ndim > \u001b[32m2\u001b[39m:\n\u001b[32m    497\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mx and y can be no greater than 2D, but have \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    498\u001b[39m                      \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mshapes \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mValueError\u001b[39m: x and y must have same first dimension, but have shapes (4920,) and (0,)"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x1200 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "analyze_lopo_results(results, patient_ids)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"LOPO Cross-Validation Complete!\")\n",
    "print(\"=\" * 80)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
